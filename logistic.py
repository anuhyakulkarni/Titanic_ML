# -*- coding: utf-8 -*-
"""CAPSTONE.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IL-gWh8WJIkWKKaX7GZLbbZ3R_U_ytpE

# TITANIC :  LEARNING FROM DISASTER

The following dataset has been taken from kaggle. 
https://www.kaggle.com/c/titanic/data

Dataset description:The dataset has a set of 12 columns ,namely:passenger ID, passenger class, name, sex, age, sibsp,parch,ticket,fare,embarked and survived.Rest of the info will be follow as we go...

Importing the required libraries: Numpy(for mathematical calculations),Pandas(),Matplot lib(for graphs), OS(to upload the saved .csv),Seaborn(for visualisation tools)  and Sklearn( for training tools)
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import os
import seaborn as sns
from sklearn.model_selection import train_test_split

from google.colab import files
files.upload()

titanic= pd.read_csv("train.csv") #Saving the csv file into a dataframe "titanic"

titanic.head()

"""After looking at the head and the desription, It is observed that the dataset has 12 columns :passengerId,name, ticket ,parch,cabin,sex,age
,SibSp( if the passenger has a sibling or a spouse),
embarked(the station where passengers embarked the ship :priceton,)
,Pclass(1,2,3)

survived column is the target

#we use seaborn tools for some visualisation of all data available
"""

#heatmap null for checking if the dataframe has any null values
sns.heatmap(titanic.isnull())

"""**we use seaborn tools for some visualisation of all data available**"""

# The graph shows the probability of no of deaths to survival (death:survival::0:1)
sns.countplot(x='Survived',data=titanic)

#The graph shows the probability of no of deaths to survival OF men and women in the ship
sns.countplot(x='Survived',hue='sex',data=titanic)

#The graph shows the probability of no of deaths to survival of different passenger classes
sns.countplot(x='Survived',hue='Pclass',data=titanic)

sns.pairplot(titanic,hue='Survived')

"""the features are compared using PEASON CORRELATION , to enable easy judgement of higher significant features"""

data2 = titanic.corr('pearson')
data2

#absolute value of correlation
 abs(data2.loc['Survived']).sort_values(ascending=False)

sns.distplot(titanic['Age'])

"""#dropping of null/insignificant features

Most of the rows(values) in cabin column are empty,so the column is dropped(deleted).
"""

titanic.drop('Cabin',axis=1,inplace=True)

Male=pd.get_dummies(titanic['Sex'],drop_first=True)

embark=pd.get_dummies(titanic['Embarked'],drop_first=True)

titanic.drop('Embarked',axis=1,inplace=True)

titanic=pd.concat([titanic,Male,embark],axis=1)

titanic.drop(['Name','Sex','Ticket','PassengerId'],axis=1,inplace=True)

titanic.drop(['Age'],axis=1,inplace=True)

titanic.dropna()

titanic.head()

sns.heatmap(titanic.isnull())

"""#TRAINING THE MODEL"""

X=titanic.drop('Survived',axis=1)
Y=titanic['Survived']

x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2)

"""***import logistic regression model from sklearn, to train models ***"""

from sklearn.linear_model import LogisticRegression

modell=LogisticRegression()

modell

modell.fit(x_train,y_train)

pred=modell.predict(x_test)

from sklearn.metrics import accuracy_score
accuracy = accuracy_score(pred,y_test)

accuracy

from sklearn.metrics import confusion_matrix

confusion_matrix(y_test,pred)

"""***accuracy:76.5 %***"""